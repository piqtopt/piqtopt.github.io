
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>PI-QT-Opt</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://piqtopt.github.com/img/piqtopt.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://piqtopt.github.com/"/>
    <meta property="og:title" content="PI-QT-Opt" />
    <meta property="og:description" content="Project page for PI-QT-Opt: Predictive Information Improves Multi-Task Robotic Reinforcement Learning at Scale" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="PI-QT-Opt" />
    <meta name="twitter:description" content="Project page for PI-QT-Opt: Predictive Information Improves Multi-Task Robotic Reinforcement Learning at Scale." />
    <meta name="twitter:image" content="https://piqtopt.github.com/img/piqtopt.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                PI-QT-Opt: Predictive Information Improves</br>  Multi-Task Robotic Reinforcement Learning at Scale </br> 
                <small>
                    CoRL 2022
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
			<a href="https://kuanghuei.github.io/">
                          Kuang-Huei Lee
			</a>
                    </li>
                    <li>
                        <a href="https://tedxiao.me/">
                            Ted Xiao
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=ncJWfs0AAAAJ">
                          Adrian Li
                        </a>
                    </li>
                    <li>
			<a href="https://scholar.google.com/citations?user=SzHPa90AAAAJ">
                          Paul Wohlhart
			</a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=Z63Zf_0AAAAJ">
                          Ian Fischer
                        </a>
                    </li>
 	            <li>
                        <a href="https://scholar.google.com/citations?user=OI7zFmwAAAAJ">
                          Yao Lu
                        </a>
                    </li>
                    <br><br>
                    <a href="http://g.co/robotics">
                      <img src="img/google_brain.png" height="40px"> Google Brain</a>                    
                    <a href="http://g.co/robotics">
                      <img src="img/robotics-at-google.png" height="40px"> Robotics at Google</a>
                    <a href="https://everydayrobots.com/">
                      <img src="img/EverydayRobots2.gif" height="40px"> Everyday Robots</a> <br><br>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/pdf/2210.08217.pdf">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/ioMplI5HlZQ">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Supplementary Video</strong></h4>
                            </a>
                        </li>
                        <!-- li>
                            <a href="https://ai.googleblog.com/2021/04/multi-task-robotic-reinforcement.html">
                            <image src="img/google-ai-blog-small.png" height="60px">
                                <h4><strong>Blogpost</strong></h4>
                            </a>
                        </li -->
                        <!-- <li>
                            <a href="https://github.com/">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p style="text-align:center;">
<!--        	    <image src="img/collage_v2.gif" class="img-responsive">-->
                </p>
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    The <i>predictive information</i>, the mutual information between the past and future, has been shown to be a useful representation learning auxiliary loss for training reinforcement learning agents, as the ability to model what will happen next is critical to success on many control tasks. While existing studies are largely restricted to training specialist agents on single-task settings in simulation, in this work, we study modeling the predictive information for robotic agents and its importance for general-purpose agents that are trained to master a large repertoire of diverse skills from large amounts of data. Specifically, we introduce Predictive Information QT-Opt (PI-QT-Opt), a QT-Opt agent augmented with an auxiliary loss that learns representations of the predictive information to solve up to 297 vision-based robot manipulation tasks in simulation and the real world with a single set of parameters. We demonstrate that modeling the predictive information significantly improves success rates on the training tasks and leads to better zeroshot transfer to unseen novel tasks. Finally, we evaluate PI-QT-Opt on real robots, achieving substantial and consistent improvement over QT-Opt in multiple experimental settings of varying environments, skills, and multi-task configurations.

                </p>
            </div>
        </div>



        <!--div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/i3uUGSko2zY" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Approach
                </h3>
                <p class="text-justify">
                    PI-QT-Opt combines a predictive information auxiliary similar to that introduced in PI-SAC with the QT-Opt architecture. We define the past (X) to be the current state and action, (s, a), and the future (Y) to be the next state, next optimal action, and reward, (s_0, a0, r). A state s includes an RGB image observation and proprioceptive information. Image observations are processed by a simple conv net, the output of which is mixed with action, proprioceptive state, and the current task context using additive conditioning. A second simple conv net processes the combined state representation. All of the convolutional parameters are shared by both the forward encoder e_θ for modeling the predictive information and the Q-function Q_θ, but the shared representation output from this is further processed by separate MLPs, to allow each loss to specialize its representation as needed, while still allowing the predictive information loss to influence the shared convolutional representation. Not shown in Figure 1 is that the target Q-function and backward encoder for modeling the predictive information also share the same base lagged and non-trainable convolutional representation, but the backward encoder has its own trainable MLP, in order to learn any differences in dynamics when trying to predict the past from the future, rather than predicting the future from the past, as the forward encoder does. In addition, we concatenate the convolutional representation with observed reward r(s, a) as the input to the backward encoder MLP head.
                </p>
                <p style="text-align:center;">
                    <image src="img/piqtopt.png"  class="img-responsive" height="600px">
                </p>
                <p class="text-justify">
                    We find that adding a predictive information auxiliary loss is an easy way to give substantial performance improvements to our chosen RL algorithm, as in Lee et al. which introduced Predictive Information Soft Actor-Critic (PI-SAC). However, we note that PI-SAC on its own was unable to solve our tasks, yielding close-to-zero success rates, and neither was SAC, which may indicate that the choice of base RL algorithm is still critical.
                </p>
                <br>
                <p class="text-justify">
                    In order to learn one general-purpose agent for multiple tasks, we condition the Q-functions and the Predictive Information auxiliary on a task context, which describes the specific task that we wish the agent to perform. In our setting, a task involves a robot skill and a set of objects that the robot should interact with. We use two practical implementations of task context in different robot manipulation settings. One is image-based, where a task is specified with the initial image, the initial object locations, and the skill type. It only considers locations and skill types and thus could enable good generalization across different and even novel objects. The other one is language-based, where tasks are specified with natural language, similar to BC-Z.
                </p>
                <p style="text-align:center;">
                    <image src="img/full_architecture.png"  class="img-responsive" height="600px">
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Results
                </h3>       
                <p class="text-justify">
                    To analyze how PI-QT-Opt compares with QT-Opt across different multi-task robotic learning scenarios, we explore a variety of challenging simulation and real vision-based robotic manipulation environments. While prior results on large-scale robotic grasping focused on a limited set of tasks, we verify the robustness and scalability of PI-QT-Opt by studying many different environments across hundreds of different tasks in the real world. Specifically, we study 6 different multi-task, vision-based robotic manipulation settings in 3 different environments in simulation. Four of the manipulation settings have the corresponding hardware setup permitting real-world evaluation.
                </p>
                <p style="text-align:center;">
                    <image src="img/envs.png"  class="img-responsive" height="600px">
                </p>
                <p class="text-justify">
                    We find that PI-Qt-Opt is consistently stronger in all domains compared to a baseline QT-Opt method, including in challenging real world evaluation scenarios.
                </p>
                <p style="text-align:center;">
                    <image src="img/train_results.png"  class="img-responsive" height="600px">
                </p>
                <p class="text-justify">
                    A core hypothesis of this work is that the ability to model what will happen next is critical to success on control tasks. This ability can be quantified by the amount of predictive information, I(X, Y ), the agent’s representation captures. We analyze the SayCan 300-task PI-QT-Opt model to compare the estimates of I(X, Y) versus TD-error for both successful and failed episodes. We can observe that the amount of predictive information is generally higher in successful episodes, and that episodes with high TD-errors have much lower predictive information and are always failures.
                </p>
                <p style="test-align:center;">
                    <image src="img/pi_vs_td.png"  class="image-responsive" width="100%">
                </p>
	       </div>
        </div>
            
        
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{lee2022pi,
  title={PI-QT-Opt: Predictive Information Improves Multi-Task Robotic Reinforcement Learning at Scale},
  author={Lee, Kuang-Huei and Xiao, Ted and Li, Adrian and Wohlhart, Paul and Fischer, Ian and Lu, Yao},
  journal={arXiv preprint arXiv:2210.08217},
  year={2022}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The authors would like to thank Alex Herzog, Mohi Khansari, Daniel Kappler, Peter Pastor for adapting infrastructure and algorithms for the image-based task context from generic to instance specific grasping, Sangeetha Ramesh for leading robot operations for data collection and evaluations for the VILD model training, and Kim Kleiven for leading the waste sorting service project that constitutes the framework for training and deployment of the instance grasping task set, including defining benchmark and protocol. We thank Jornell Quiambao, Grecia Salazar, Jodilyn Peralta, Justice Carbajal, Clayton Tan, Huong T Tran, Emily Perez, Brianna Zitkovich and Jaspiar Singh for helping administrate real-world robot experiments. We would also like to thank Sergio Guadarrama and Karol Hausman for valuable feedback.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
